{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science Homework 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this homework we'll be looking at the data released by the State Bank of Pakistan. The data is the daily bank-wise and donor-wise receipts of the fund for the Daimer Bhasha and Mohmand Dam. You can find them in the following link: http://www.sbp.org.pk/notifications/FD/DamFund/Damfund.htm. Take a moment to look around the data and try to figure out what the possible challenges could be.\n",
    "\n",
    "The main purpose of this homework is to teach how to scrape data from the web, clean it, and import it into Pandas for data analysis purposes. There are however some things to note:\n",
    "1. As you can tell, the data is in PDF form. PDF is the most difficult to handle data format and if you get extremely broken CSV files, there isn't a need to worry, that's where the cleaning part comes in.\n",
    "2. We'll be using an API to convert the data from PDF to CSV, and then from CSV to Pandas. There are, however, other ways to do this. The reason we wanted to do this method is two-fold\n",
    "    * It will teach you how to communicate with APIs using Python, which will be a useful skill when you want to deploy your data models as an API so that it can work with other APIs that need those data models. Moreover, a lot of data you get in the real world is from APIs. \n",
    "    * The CSV will be extremely inconsistent, so it will give you immense practice with using regular-expressions, which are extremely important in the Data Science tool-kit.\n",
    "    \n",
    "Submit the notebooks in a similar format to the Labs: print the relevant output in each cell **only if it has an output. The initial scraping and converting does not have any output**, and name the notebooks as:\n",
    "**rollnumber_HW1.ipynb** for e.g **20100237_H1.ipynb**\n",
    "\n",
    "Please make sure you complete full parts (denoted by a Header each in this notebook) as the grading will be based on parts. Needless to say, do not copy someone else's code. In most Data Science careers, the main skill is not how good you are at coding, but how well you are able to use the tools at your disposal and what inferences you are able to make with the information that you have. Thus, while you might be able to do the HW by looking at someone else's code, unless you go through the actual thought process, you won't learn a lot.\n",
    "\n",
    "We'll be using a lot of libraries in this tutorial, make sure you go through them so you understand what they are used for.\n",
    "\n",
    "**NOTE: If you are more comfortable doing so, as I am, you can do the assignment on your preferred text editor on simple Python and then write the code neatly in a notebook.** Personally, I find Sublime/Vim easier to use than Jupyter, mostly since a lot of shortcuts there make coding much easier, while here the shortcuts are more about navigation and controlling your cells.\n",
    "\n",
    "**The homework is to be done in pairs of 2.** \n",
    "\n",
    "**Naming convention: rollnumber1_rollnumber2_HW1.ipynb**\n",
    "\n",
    "Total Marks: 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0: Getting the Data\n",
    "\n",
    "You can have a look at the data through the link given above. Download a few PDF files and go through the data to see what it looks like. How many columns are there, each, in the PDF files? Are there any inconsistencies? Any particular values that pop out that would need to be taken care of later in your cleaning? Think of all these questions when going through the initial PDF because they will prove really helpful when you can not figure out why there are so many \"NaN\" values in your final DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Data Scraping              \n",
    "Marks: 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be using what is called the *requests* model to get an HTML page, and then use *BeautifulSoup* to parse that HTML page such that we are able to to derive the appropriate information from it. I recommend you go through the documentation of each to learn more about how to use the libraries. \n",
    "\n",
    "* [Requests Documentation](http://docs.python-requests.org/en/master/)\n",
    "* [BeautifulSoup Documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "* [BeautifulSoup + Requests Tutorial](https://www.pythonforbeginners.com/python-on-the-web/web-scraping-with-beautifulsoup)\n",
    "* [BeautifulSoup](https://medium.freecodecamp.org/how-to-scrape-websites-with-python-and-beautifulsoup-5946935d93fe) Note that this tutorial is more detailed. I would highly recommend you go through this as well even though the library used here is urllib2 instead of requests (which you can do as well!). It also links to more web-scraping libraries like Scrapy for more complicated scraping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import requests\n",
    "import urllib2\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import re\n",
    "\n",
    "# os is being imported so you can make a new directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Soban Ali\\Anaconda2\\lib\\site-packages\\bs4\\__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 174 of the file C:\\Users\\Soban Ali\\Anaconda2\\lib\\runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n",
      "08-10-2018\n",
      "05-10-2018\n",
      "04-10-2018\n",
      "03-10-2018\n",
      "02-10-2018\n",
      "01-10-2018\n",
      "28-09-2018\n",
      "27-09-2018\n",
      "26-09-2018\n",
      "25-09-2018\n",
      "24-09-2018\n",
      "19-09-2018\n",
      "18-09-2018\n",
      "17-09-2018\n",
      "14-09-2018\n",
      "13-09-2018\n",
      "12-09-2018\n",
      "11-09-2018\n",
      "10-09-2018\n",
      "07-09-2018\n",
      "06-09-2018\n",
      "05-09-2018\n",
      "04-09-2018\n",
      "03-09-2018\n",
      "31-08-2018\n",
      "30-08-2018\n",
      "29-08-2018\n",
      "28-08-2018\n",
      "27-08-2018\n",
      "24-08-2018\n",
      "20-08-2018\n",
      "17-08-2018\n",
      "16-08-2018\n",
      "15-08-2018\n",
      "13-08-2018\n",
      "10-08-2018\n",
      "09-08-2018\n",
      "08-08-2018\n",
      "07-08-2018\n",
      "06-08-2018\n",
      "03-08-2018\n",
      "02-08-2018\n",
      "01-08-2018\n",
      "31-07-2018\n",
      "30-07-2018\n",
      "27-07-2018\n",
      "26-07-2018\n",
      "24-07-2018\n",
      "23-07-2018\n",
      "20-07-2018\n",
      "19-07-2018\n",
      "18-07-2018\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-3f2367380062>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0murls\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m     \u001b[0mlink\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murls\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m     \u001b[1;32mprint\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpdffolder_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'.pdf'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Soban Ali\\Anaconda2\\lib\\site-packages\\requests\\api.pyc\u001b[0m in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'allow_redirects'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'get'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Soban Ali\\Anaconda2\\lib\\site-packages\\requests\\api.pyc\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[1;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Soban Ali\\Anaconda2\\lib\\site-packages\\requests\\sessions.pyc\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    506\u001b[0m         }\n\u001b[0;32m    507\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 508\u001b[1;33m         \u001b[0mresp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    509\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    510\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Soban Ali\\Anaconda2\\lib\\site-packages\\requests\\sessions.pyc\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    616\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    617\u001b[0m         \u001b[1;31m# Send the request\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 618\u001b[1;33m         \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    619\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    620\u001b[0m         \u001b[1;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Soban Ali\\Anaconda2\\lib\\site-packages\\requests\\adapters.pyc\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    438\u001b[0m                     \u001b[0mdecode_content\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    439\u001b[0m                     \u001b[0mretries\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_retries\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 440\u001b[1;33m                     \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    441\u001b[0m                 )\n\u001b[0;32m    442\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Soban Ali\\Anaconda2\\lib\\site-packages\\urllib3\\connectionpool.pyc\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    599\u001b[0m                                                   \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout_obj\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    600\u001b[0m                                                   \u001b[0mbody\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 601\u001b[1;33m                                                   chunked=chunked)\n\u001b[0m\u001b[0;32m    602\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    603\u001b[0m             \u001b[1;31m# If we're going to release the connection in ``finally:``, then\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Soban Ali\\Anaconda2\\lib\\site-packages\\urllib3\\connectionpool.pyc\u001b[0m in \u001b[0;36m_make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    378\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    379\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Python 2.7, use buffering of HTTP responses\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 380\u001b[1;33m                 \u001b[0mhttplib_response\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    381\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Python 2.6 and older, Python 3\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    382\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Soban Ali\\Anaconda2\\lib\\httplib.pyc\u001b[0m in \u001b[0;36mgetresponse\u001b[1;34m(self, buffering)\u001b[0m\n\u001b[0;32m   1119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1120\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1121\u001b[1;33m             \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1122\u001b[0m             \u001b[1;32massert\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwill_close\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0m_UNKNOWN\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_CS_IDLE\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Soban Ali\\Anaconda2\\lib\\httplib.pyc\u001b[0m in \u001b[0;36mbegin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    436\u001b[0m         \u001b[1;31m# read until we get a non-100 response\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    437\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 438\u001b[1;33m             \u001b[0mversion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    439\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    440\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Soban Ali\\Anaconda2\\lib\\httplib.pyc\u001b[0m in \u001b[0;36m_read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    392\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    393\u001b[0m         \u001b[1;31m# Initialize with Simple-Response defaults\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 394\u001b[1;33m         \u001b[0mline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    395\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    396\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"header line\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Soban Ali\\Anaconda2\\lib\\socket.pyc\u001b[0m in \u001b[0;36mreadline\u001b[1;34m(self, size)\u001b[0m\n\u001b[0;32m    478\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    479\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 480\u001b[1;33m                     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_rbufsize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    481\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    482\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mEINTR\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## Write code here that will:\n",
    "    # Open each PDF link\n",
    "    # Save the PDF in a directory in the same folder \n",
    "\n",
    "pdffolder_name = \"all_pdfs\"\n",
    "    \n",
    "if not os.path.exists(pdffolder_name):    \n",
    "    os.mkdir(pdffolder_name)\n",
    "    \n",
    "    \n",
    "web = \"http://www.sbp.org.pk/notifications/FD/DamFund/Damfund.htm\"\n",
    "\n",
    "r = requests.get(web)\n",
    "data = r.text\n",
    "\n",
    "soup = BeautifulSoup(data)\n",
    "\n",
    "urls = []\n",
    "for link in soup.find_all('a'):\n",
    "    temp = link.get('href')\n",
    "    if \"pdf\" in temp:\n",
    "        urls.append(\"http://www.sbp.org.pk/notifications/FD/DamFund/\" + temp)\n",
    "    \n",
    "print len(urls)\n",
    "\n",
    "# Your code goes here #\n",
    "\n",
    "length = len(urls)\n",
    "for x in range(0, length):\n",
    "    start = urls[x].rfind('/')\n",
    "    end = urls[x].rfind('.')\n",
    "    name = urls[x][start+1:end]\n",
    "    \n",
    "    link = requests.get(urls[x])\n",
    "    print name\n",
    "    with open(os.path.join(pdffolder_name, name + '.pdf'), 'wb')as f:\n",
    "        f.write(link.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Converting from PDF to CSV\n",
    "Marks: 15\n",
    "\n",
    "You have two possible options between deciding what API to use for the conversion task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first option is communicating with an API called [Zamzar](https://www.zamzar.com) to send each PDF, ask them to convert it into CSV, and then download the converted CSV. They provide sample code to do everything from generating a simple request to starting a conversion job, checking for completion, and then downloading the finished file. You can find this information on the [Zamzar Documentation](https://developers.zamzar.com/docs) page.\n",
    "\n",
    "**Important Information: **\n",
    "\n",
    "The API only provides 100 points of free conversion, and each PDF to CSV conversion costs 3 points, that means with one account you can only convert **33** PDFs. However, this also means you have very little room to play around with this API, unless you have an extra email-address, so you need to be very careful when coding to communicate with this API. \n",
    "\n",
    "Moreover, the API only keeps the converted files for one day with a free account, so make sure you do this part in one go.\n",
    "\n",
    "**Note: Using the Zamzar API grants a bonus of 10 marks. This will help if you are not able to complete this assignment, or it can be used up in a later assignment if you get 110/100 marks in this one.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another extremely simple API is the [PDF Tables](https://pdftables.com) API which is much simpler to use than the Zamzar API, however does not allow you to check the job for completion or for any intermediate steps. Moreover, this requires the installation of a library. Once again, they allow only 50 versions for free, but that is enough conversions for us. This [blog post](https://pdftables.com/blog/pdf-to-excel-with-python) will help you figure out how to convert the PDF to CSV using Python.\n",
    "\n",
    "The cons of this API is that it will not really teach you any proper API communcation through requests since you do not have to navigate through any requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests.auth import HTTPBasicAuth\n",
    "\n",
    "'''import config'''\n",
    "import glob\n",
    "\n",
    "api_key = 'cfaa19ddf2e64b10acd95cb2f182d1f3fa79d4c4'\n",
    "\n",
    "api_key_soban = 'b4c7b9722e3696de14e3eb3c92e2d37d36bc4dd5'\n",
    "\n",
    "endpoint_post = \"https://api.zamzar.com/v1/v1/jobs\"\n",
    "\n",
    "endpoint_download = \"https://sandbox.zamzar.com/v1/files/{}/content\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./all_pdfs\\01-08-2018.pdf\n",
      "{u'errors': [{u'message': u'API key was missing or invalid', u'code': 20}]}\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'id'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-77ac28d39e6d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[1;32mprint\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m         \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"id\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m         \u001b[0mjob_ids\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[0mcheck\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'id'"
     ]
    }
   ],
   "source": [
    "pdfs_folder = './all_pdfs/*.pdf'\n",
    "\n",
    "\n",
    "# You need a list to store all the job_ids from the response of posting the conversion job, \n",
    "# if you are using the Zamzar API\n",
    "\n",
    "job_ids = []\n",
    "\n",
    "half = length/2\n",
    "\n",
    "target_format = \"csv\"\n",
    "source_files = []\n",
    "flags = []\n",
    "\n",
    "check = 0\n",
    "\n",
    "# This piece of code shows you what glob does\n",
    "for file_name in glob.glob(pdfs_folder):\n",
    "    source_files.append(file_name)\n",
    "    print file_name\n",
    "    endpoint = \"https://sandbox.zamzar.com/v1/jobs\"\n",
    "    \n",
    "    ## Write code here to post a job, and append each job's id into job_ids ##\n",
    "    if (check <= half):\n",
    "        file_content = {'source_file': open(file_name, 'rb')}\n",
    "        data_content = {'target_format': target_format}\n",
    "        res = requests.post(endpoint, data=data_content, files=file_content, auth=HTTPBasicAuth(api_key, ''))\n",
    "        data = res.json()\n",
    "        print data\n",
    "        job = data[\"id\"]\n",
    "        job_ids.append(job)\n",
    "        check = check + 1\n",
    "    elif check != length:\n",
    "        file_content = {'source_file': open(file_name, 'rb')}\n",
    "        data_content = {'target_format': target_format}\n",
    "        res = requests.post(endpoint, data=data_content, files=file_content, auth=HTTPBasicAuth(api_key_soban, ''))\n",
    "        data = res.json()\n",
    "        print data\n",
    "        job = data[\"id\"]\n",
    "        job_ids.append(job)\n",
    "        check = check + 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below this cell write the code to download the completed files. First check if a job_id's status is completed and wait until it is. After it has been completed, download the file and save it.\n",
    "\n",
    "The exact code required here is all in the documentation, the only additional task you have to do on your own is figure out a way to find out which file has just been received from the job_id, and name the local file.\n",
    "\n",
    "**Please look at the Example JSON response in the [documentation](https://developers.zamzar.com/docs) to learn how to figure out the filenames, job status etc**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-1d443898c404>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     77\u001b[0m                             \u001b[0mcount\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcount\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m                     \u001b[1;32mexcept\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m                         \u001b[1;32mprint\u001b[0m \u001b[1;34m'Error'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     80\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcount\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mlength\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## Your code goes here ##\n",
    "\n",
    "length_job_ids = len(job_ids)\n",
    "for x in range(0, length_job_ids):\n",
    "    flags.append(0)\n",
    "\n",
    "print length_job_ids\n",
    "print len(flags)\n",
    "\n",
    "csvfolder_name = 'all_csvs'\n",
    "\n",
    "if not os.path.exists(csvfolder_name):    \n",
    "    os.mkdir(csvfolder_name)\n",
    "\n",
    "\n",
    "count = 0\n",
    "while(True):\n",
    "    for x in range(0, length_job_ids):\n",
    "        if flags[x] != 1:\n",
    "            if count <= half:\n",
    "                job_id = job_ids[x]\n",
    "                print job_id\n",
    "                endpoint_status = \"https://sandbox.zamzar.com/v1/jobs/{}\".format(job_id)\n",
    "                # get status of job\n",
    "                response = requests.get(endpoint_status, auth=HTTPBasicAuth(api_key, ''))\n",
    "                data = response.json()\n",
    "                print data\n",
    "                # if job is successful, then download\n",
    "                if data['status'] == 'successful':\n",
    "                    target_id = data['target_files'][0]['id']\n",
    "                    target_name = data['target_files'][0]['name']\n",
    "\n",
    "                    local_filename = os.path.join(csvfolder_name, target_name)\n",
    "                    endpoint_download = \"https://sandbox.zamzar.com/v1/files/{}/content\".format(target_id)\n",
    "\n",
    "                    response = requests.get(endpoint_download, stream=True, auth=HTTPBasicAuth(api_key, ''))\n",
    "\n",
    "                    try:\n",
    "                        with open(local_filename, 'wb') as f:\n",
    "                            for chunk in response.iter_content(chunk_size=1024):\n",
    "                                if chunk:\n",
    "                                    f.write(chunk)\n",
    "                                    f.flush\n",
    "\n",
    "                            print 'File downloaded'\n",
    "                            flags[x] = 1\n",
    "                            count = count + 1\n",
    "                    except IOError:\n",
    "                        print 'Error'\n",
    "            else:\n",
    "                job_id = job_ids[x]\n",
    "                print job_id\n",
    "                endpoint_status = \"https://sandbox.zamzar.com/v1/jobs/{}\".format(job_id)\n",
    "                # get status of job\n",
    "                response = requests.get(endpoint_status, auth=HTTPBasicAuth(api_key_soban, ''))\n",
    "                data = response.json()\n",
    "                print data\n",
    "                # if job is successful, then download\n",
    "                if data['status'] == 'successful':\n",
    "                    target_id = data['target_files'][0]['id']\n",
    "                    target_name = data['target_files'][0]['name']\n",
    "\n",
    "                    local_filename = os.path.join(csvfolder_name, target_name)\n",
    "                    endpoint_download = \"https://sandbox.zamzar.com/v1/files/{}/content\".format(target_id)\n",
    "\n",
    "                    response = requests.get(endpoint_download, stream=True, auth=HTTPBasicAuth(api_key_soban, ''))\n",
    "\n",
    "                    try:\n",
    "                        with open(local_filename, 'wb') as f:\n",
    "                            for chunk in response.iter_content(chunk_size=1024):\n",
    "                                if chunk:\n",
    "                                    f.write(chunk)\n",
    "                                    f.flush\n",
    "\n",
    "                            print 'File downloaded'\n",
    "                            flags[x] = 1\n",
    "                            count = count + 1\n",
    "                    except IOError:\n",
    "                        print 'Error'\n",
    "\n",
    "    if count == length:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Parsing the CSV File\n",
    "Marks: 35\n",
    "\n",
    "This is perhaps the most difficult part of the assignment, you have to follow a similar strategy to what you did the Udacity Lab 1. You can not simply use Pandas read_csv since the conversion is not perfect and there will be rows with different number of columns, which Pandas does not take care of.\n",
    "\n",
    "### **Main Task:**\n",
    "* Write a function that parses a CSV into a Pandas DataFrame\n",
    "* Each DataFrame should consist of three columns with headers Bank, Donor_Name, and Amount\n",
    "* The date should be retrieved from the given filename \n",
    "* The Donor_Name can be NaN, as it is in a lot of cases. But try to retrieve as much information as possible\n",
    "* Remove all \"Page of\" rows\n",
    "* Don't include the header rows (e.g. \"SUPREME COURT FUND....\") into the DataFrame\n",
    "* The Amount should be converted into a Pandas numeric at the end\n",
    "\n",
    "### Other info:\n",
    "Some important resources for this part are (you can choose any one tutorial that you feel is easy to understand, they all cover roughly the same content):\n",
    "* [RegEx Tutorial 1](https://www.regular-expressions.info/)\n",
    "* [RegEx Tutorial 2](https://regexone.com/lesson/introduction_abcs)\n",
    "* [RegEx Tutorial 3](https://www.rexegg.com/)\n",
    "* [RegEx Cheatsheat](https://medium.com/factory-mind/regex-tutorial-a-simple-cheatsheet-by-examples-649dc1c3f285)\n",
    "* This [RegEx Editor](https://regex101.com/) is your best friend since you can test your expression separately on this\n",
    "\n",
    "You will probably have to use the CSV reader in order to get all the rows of the file. You can learn more about it using this [tutorial](https://www.alexkras.com/how-to-read-csv-file-in-python/).\n",
    "\n",
    "Some tips:\n",
    "* First find out how many columns are in each row\n",
    "* Print out rows which are longer than they should be (they should all be of length 3)\n",
    "* Try to find patterns in how the data is spread, and what common problems exist in all rows\n",
    "* Write some regex to try an extract the amount from the problem row and then:\n",
    "    * Put the amount as the third column\n",
    "    * Merge the rest of the string as a name of the donor in the 2nd column\n",
    "* Also check if the rows with 3 columns are correctly formatted or not, many of them would probably not be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import re # To use regular expressions\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "example_file = './all_csvs/17-08-2018.csv' # Assuming the file is in the folder all_csvs and is named appropriately\n",
    "# This is one of the most problematic files which is why I have included this in the example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def parser(filename):\n",
    "    # Complete this function\n",
    "    re_list = []\n",
    "    \n",
    "    with open(filename) as f:\n",
    "        reader = csv.reader(f)\n",
    "        data = [row for row in reader]\n",
    "        #result = p.match(data)\n",
    "        ## Just an example of one way to use the CSV module\n",
    "        data = data[3:]\n",
    "        new_data = []\n",
    "        \n",
    "        for record in data:\n",
    "            something = re.search((r'(.*)total' or r'page(.*)'), record[0], re.I)\n",
    "            #print something\n",
    "            if (something == None):\n",
    "                while '' in record:\n",
    "                    record.remove('')\n",
    "                if 'Page' not in record[0]:    \n",
    "                    if len(record) >= 1:\n",
    "                        modified_record = ['','','']\n",
    "                        bank_name = ''\n",
    "                        amount = ''\n",
    "                        name = ''\n",
    "                        #print record\n",
    "                        for i in range(0, len(record)):\n",
    "                            record[i] = record[i].strip() #removes starting and ending spaces\n",
    "                            while ',' in record[i]:\n",
    "                                record[i] = record[i].replace(',', '')\n",
    "                            \n",
    "                            #print record[i]\n",
    "                            temp2 = re.search(r'.*(bank).*', record[i], re.I)\n",
    "                            temp1 = re.search( r'^[0-9 .]*$' , record[i], re.I)\n",
    "                            if temp2:\n",
    "                                bank_name = record[i]\n",
    "                            \n",
    "                            elif temp1:\n",
    "                                if(len(record[i]) == 11) and record[i].startswith('03'):\n",
    "                                    amount = ''\n",
    "                                    \n",
    "                                elif len(record[i]) > 11:\n",
    "                                    #if record[i].startswith('00')\n",
    "                                    temp3 = record[i].split()\n",
    "                                    for i in range(0, len(temp3)):\n",
    "                                        if '.' in temp3[i]:\n",
    "                                            amount = temp3[i]\n",
    "                                            \n",
    "                                else:\n",
    "                                    am = record[i].split()\n",
    "                                    for x in am:\n",
    "                                        if not (x.startswith('0')):\n",
    "                                            amount = x\n",
    "                                \n",
    "                            else:\n",
    "                                name = name + ' ' +record[i]\n",
    "                                #test = re.search( r'^[0-9 .]*$' , name, re.I)\n",
    "                                #if test:\n",
    "                                 #   name = ''\n",
    "                            \n",
    "                            if(i == len(record)-1):\n",
    "                                modified_record[0] = bank_name\n",
    "                                modified_record[1] = name.strip()\n",
    "                                modified_record[2] = amount\n",
    "                                #print modified_record\n",
    "                                new_data.append(modified_record)\n",
    "                        #while ',' in record[1]:\n",
    "                        #    record[1] = record[1].replace(',', '')\n",
    "\n",
    "                        #if (len(record)>=3):\n",
    "                        #    while ',' in record[2]:\n",
    "                        #        record[2] = record[2].replace(',', '')\n",
    "\n",
    "                        # if the second one is number, then insert an empty string on index 1.\n",
    "                        \n",
    "                        #temp1 = re.search( r'^[0-9 ]*$' , record[1], re.I)\n",
    "                        #if not (temp1 == None):\n",
    "                        #    record.insert(1, '')\n",
    "\n",
    "                        #remove records whose length is only 1\n",
    "                        #except for the bank string and number string, concat all others\n",
    "                        \n",
    "                        #print record\n",
    "                        #temp2 = re.search(r'.*(bank).*', record[0], re.I)\n",
    "                        #if temp2:\n",
    "                        #    new_data.append(record)\n",
    "        return new_data\n",
    "\n",
    "    \n",
    "#print len(parser('02-08-2018.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember, remove headers and convert all amounts to Numeric; if it can't be converted it needs to be NaN\n",
    "\n",
    "def read_csv(filename):\n",
    "    headers = ['Bank', 'Donor_Name','Amount']\n",
    "    \n",
    "    raw_data = parser(filename)\n",
    "    \n",
    "    df = pd.DataFrame(raw_data)\n",
    "    # Your code goes here\n",
    "    df.columns = headers\n",
    "    df = df.replace('', np.nan, regex=True)\n",
    "    \n",
    "    #check_list = df['Bank'].values\n",
    "    #list_size = len(check_list)\n",
    "    #for x in range(0, list_size):\n",
    "        #if len(check_list[x]) > 11:\n",
    "    #        print check_list[x]\n",
    "    df['Amount'] = df['Amount'].apply(pd.to_numeric)\n",
    "\n",
    "    \n",
    "    return df\n",
    "    \n",
    "#print read_csv(example_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Importing Full Dataset\n",
    "Marks: 10 \n",
    "\n",
    "The only additional task in this part is to:\n",
    "* Run the parser on all the files\n",
    "* For each file **add a 'Date' column, which should be inferred from the filename**\n",
    "* Concatenate each DataFrame into one large DataFrame. *Hint: concat*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            Bank        Donor_Name  Amount        Date\n",
      "0  AL BARAKA BANK (PAKISTAN) LTD       FARHAN ARIF  5000.0  01-08-2018\n",
      "1  AL BARAKA BANK (PAKISTAN) LTD       DAM UL HUDA  2000.0  01-08-2018\n",
      "2  AL BARAKA BANK (PAKISTAN) LTD       FARIS AHMED  2000.0  01-08-2018\n",
      "3  AL BARAKA BANK (PAKISTAN) LTD    MUHAMMAD AJMAL  1000.0  01-08-2018\n",
      "4  AL BARAKA BANK (PAKISTAN) LTD  SHAHEENA SULTANA  1000.0  01-08-2018\n",
      "(174741, 4)\n"
     ]
    }
   ],
   "source": [
    "files = glob.glob('./all_csvs/*.csv')\n",
    "\n",
    "csv_files = []\n",
    "for file_name in files:\n",
    "    csv_files.append(file_name)\n",
    "    #print file_name\n",
    "    \n",
    "frames = []  \n",
    "def dataframe(filelist):\n",
    "    length_filelist = len(filelist)\n",
    "    for x in range(0, length_filelist):\n",
    "        start = csv_files[x].rfind('\\\\')\n",
    "        end = csv_files[x].rfind('.')\n",
    "        \n",
    "        date = csv_files[x][start+1:end]\n",
    "        \n",
    "        \n",
    "        df = read_csv(filelist[x])\n",
    "        df['Date'] = date\n",
    "        frames.append(df)\n",
    "        \n",
    "        # after calling read_csv on a particular csv, append that dataframe to frames list\n",
    "        \n",
    "    result = pd.concat(frames)\n",
    "    return result\n",
    "\n",
    "full_data = dataframe(files)\n",
    "\n",
    "print full_data.head()\n",
    "#print full_data['Amount'].sum()\n",
    "print full_data.shape\n",
    "#print full_data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Data Integrity Checks\n",
    "Marks: 20\n",
    "\n",
    "* How many NaN values are there in each column? Why are they there? \n",
    "* What are the maximum and minimum values, is there anything peculiar about the max values?\n",
    "* Are there any rows which are not NaN but should still be a different DataFrame altogether?\n",
    "* Should these problem rows be removed? Can they be useful in other ways?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN in Bank: 8707\n",
      "NaN in Donor_Name: 6431\n",
      "NaN in Amount: 7401\n"
     ]
    }
   ],
   "source": [
    "print 'NaN in Bank: ' + str(full_data['Bank'].isna().sum())\n",
    "print 'NaN in Donor_Name: ' + str(full_data['Donor_Name'].isna().sum())\n",
    "print 'NaN in Amount: ' + str(full_data['Amount'].isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NaN in Bank column represents that particular row lacks bank name when its data was added in pdf table of dam funds. There's surely missing places in data where bank name isn't available. But that's not necessarily true. Large number of Nan bank is cuz we aren't able to extract bank names from donor names columns, in order to append it to bank column\n",
    "\n",
    "Nan in Donor_Name is due to missing names in record. Large number of NaNs is cuz we haven't handled cases where donor details are appended in bank column. Extracting donor details from there and adding them to respective columns will decrease the number of NaNs\n",
    "\n",
    "Nans in Amount shows either donation hasn't been made which means its a faulty entry. Upto our knowledge, all cases for correct donation entries in donation have been handled. Further cleaning can improve data upon finding further flaws in approach and data segregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1006452547.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "maxvalue = full_data['Amount'].max()\n",
    "print maxvalue\n",
    "minvalue = full_data['Amount'].min()\n",
    "print minvalue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Max Value: 1006452547.0\n",
    "\n",
    "Min Value: 01.0\n",
    "    \n",
    "Max value is pretty big considering its a donation. Perhaps its a collective donation from a big corporate organization. Still, it makes me question the authenticity of data provided to public by SBP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Formatting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering diversity and segreation in data, large amount of rows don't have NaN. Depending on the given data in their column,they can form multiple dataframes considering pattern in their values. Some have Names and addresses combined that can make another dataframe. Some have bank account number linked with name too. Some have mobile number combined with names. Multiple type of data is availabe which can be used to make segregated dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Formatting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It depends on the scenario of usage of data. Like if we are planning to look for properly formatted data without any additional information from raw file, then these rows can be deleted.\n",
    "Otherwise, they can be used for secondary purposes. e.g if we have bank account number, that can be used to verify the record added against that account in provided data by contacting relevant authorities. If there's sort of public survey carrying out by government, phone numbers can be used to contact respective person for their assistence regarding respective work. It can also provide data generation origin e.g we can get location of relative branch by address or bank account number. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
